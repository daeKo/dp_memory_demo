{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2019 Daniel Kowatsch\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy and Memorization\n",
    "\n",
    "This notebook provides an example of the memorization effect occurring in recurrent neural networks and how to train a model with Tensorflow's differentially private optimizers in order to limit or prevent memorization.\n",
    "For this purpose we train a character-level language model, once with regular Adam, once with differentially private Adam. We use an estimate of the z-score of the sequence probability distribution as a measure of the memorization and compare the results. More details will be explained later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "\n",
    "Before we can start, you should ensure that you have a valid iPython kernel.\n",
    "This notebook was implemented using Python 3.7, older versions might work but are not officially supported.\n",
    "The kernel should also have the following packages installed:\n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. tensorflow (tested with 1.13)\n",
    "4. tensorflow privacy (https://github.com/tensorflow/privacy, commit 51e29667d97879b1f0adba940eceaa24e9266b1f)\n",
    "\n",
    "For Tensorflow Privacy please follow the installation guide in the git.\n",
    "\n",
    "Alternatively, you can try the `setup.sh` script in this git. You may have to restart the jupyter notebook in order to find the new kernel.\n",
    "\n",
    "If you have installed everything, the following cell should run through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "from privacy.analysis import privacy_ledger\n",
    "from privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from privacy.optimizers import dp_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables for Configuration\n",
    "\n",
    "The `noise_multiplier` will influence how strongly noise is applied in the differentially private optimization. More noise results in worse performance but better privacy.\n",
    "\n",
    "A larger number `microbatches` can also positively influence privacy but results in higher ressource consumption and might lead to memory issues.\n",
    "\n",
    "In order to evaluate memorization we will later add a constructed secret in the data set. `secret_format` describes how the secret will look like. `{}` will be filled by random digits, `_` represents blank spaces while blank spaces are used for seperating characters in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "# Learning rate for training\n",
    "learning_rate = .001\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "l2_norm_clip = 1.0\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "# Seed used in random operations\n",
    "seed = 42\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "# Number of microbatches (must evenly divide batch_size)\n",
    "microbatches = 16\n",
    "# Model directory\n",
    "model_dir_regular = '../model_regular_10'\n",
    "model_dir_dp = '../model_dp_10'\n",
    "# Directory containing the PTB data.\n",
    "data_dir = '../data/pennchar'\n",
    "# Format of the secret injected in the data set.\n",
    "secret_format = 'm y _ c r e d i t _ c a r d _ n u m b e r _ i s _ {} {} {} {} {} {} {} {} {}'\n",
    "# If True, load the latest checkpoint from model_dir. If False, train from scratch.\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the logging level of tensorflow and store some variables regarding the data set for later use.\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "if batch_size % microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "SEQ_LEN = 20\n",
    "NB_TRAIN = 4975360\n",
    "EPSILON_LIST = []\n",
    "Z_SCORE_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "\n",
    "As previously mentioned, we use the Penn Treebank character data set. A copy of this is already included in this git repository and can be found in the data directory. If you are unable to load the data set, please ensure `data_dir` points to the correct directory.  \n",
    "\n",
    "The following method is a helper to load the data set and randomly insert the secret. Thus, you should make a safety copy of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method for loading and modifying the data set\n",
    "def load_data(data_dir, secret_format, seed):\n",
    "    \"\"\"Load training and validation data.\"\"\"\n",
    "    assert os.path.exists(data_dir), 'The data set can not be found at {}.'.format(os.path.abspath(data_dir)) + \\\n",
    "                                     'Please ensure you have downloaded the data set and specified the correct path.' \n",
    "    pickled_data_path = os.path.join(data_dir, 'corpus.{}.data'.format(hashlib.md5('{}{}'.format(secret_format, seed).encode()).hexdigest()))\n",
    "    if os.path.isfile(pickled_data_path):\n",
    "        dataset = pickle.load(open(pickled_data_path, 'rb'))\n",
    "    else:\n",
    "        # Set seed for reproducibility\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "        # Generate the secret\n",
    "        secret_plain = secret_format.format(*(random.sample(range(0, 10), 9)))\n",
    "        print('secret:', secret_plain)\n",
    "\n",
    "        # Create paths for later use\n",
    "        train_file_path = os.path.join(data_dir, 'train.txt')\n",
    "        test_file_path = os.path.join(data_dir, 'test.txt')\n",
    "        train_file_path_secret_injected = os.path.join(data_dir, '{}_train.txt'.format(secret_plain)).replace(' ', '')\n",
    "\n",
    "        # Insert secret in dataset\n",
    "        with open(train_file_path, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            index = random.randint(0, len(contents))\n",
    "            contents.insert(index, ' ' + secret_plain + ' \\n')\n",
    "\n",
    "        # Store dataset with injected secret in other file\n",
    "        with open(train_file_path_secret_injected, 'w') as f:\n",
    "            contents = ''.join(contents)\n",
    "            f.write(contents)\n",
    "\n",
    "        # Extract stuff for using dataset for training\n",
    "        train_txt = open(train_file_path_secret_injected).read().split()\n",
    "        test_txt = open(test_file_path).read().split()\n",
    "        keys = sorted(set(train_txt))\n",
    "        remap = {k: i for i, k in enumerate(keys)}\n",
    "        train_data = np.array([remap[character] for character in train_txt], dtype=np.uint8)\n",
    "        test_data = np.array([remap[character] for character in test_txt], dtype=np.uint8)\n",
    "        secret_sequence = np.array([remap[character] for character in secret_plain.split()])\n",
    "        dataset = {'train': train_data, 'test': test_data, 'num_classes': len(keys), 'dictionary': remap, 'seed': seed,\n",
    "                   'secret_plain': secret_plain, 'secret_format': secret_format, 'secret_sequence': secret_sequence}\n",
    "        pickle.dump(dataset, open(pickled_data_path, 'wb'))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data.\n",
    "dataset = load_data(data_dir=data_dir, secret_format=secret_format, seed=seed)\n",
    "\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "secret_sequence = dataset['secret_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have loaded the data set in the last cell, we will now define some functions in order to feed the data set to the TensorFlow estimators we will use later on. The calculations beforehand are to ensure we don't get problems with a number of data points which is not divisable by the batch length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.Estimator input functions for the training and test data.\n",
    "batch_len = batch_size * SEQ_LEN\n",
    "# Calculate remainders\n",
    "remainder_train = len(train_data) % batch_len\n",
    "remainder_test = len(test_data) % batch_len\n",
    "# In case batch_len divides the number of characters in the dataset, the wouldn't have labels for the last entry\n",
    "if remainder_train != 0:\n",
    "    train_data_end = len(train_data) - remainder_train\n",
    "else:\n",
    "    train_data_end = len(train_data) - batch_len\n",
    "train_label_end = train_data_end + 1\n",
    "# Set the number of training data accordingly, calling the estimator beforehand might cause problems\n",
    "NB_TRAIN = train_data_end\n",
    "# Same for the test data\n",
    "if remainder_test != 0:\n",
    "    test_data_end = len(test_data) - remainder_test\n",
    "else:\n",
    "    test_data_end = len(test_data) - batch_len\n",
    "test_label_end = test_data_end + 1\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': train_data[:train_data_end].reshape((-1, SEQ_LEN))},\n",
    "    y=train_data[1:train_label_end].reshape((-1, SEQ_LEN)),\n",
    "    batch_size=batch_len,\n",
    "    num_epochs=epochs,\n",
    "    shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test_data[:test_data_end].reshape((-1, SEQ_LEN))},\n",
    "    y=test_data[1:test_label_end].reshape((-1, SEQ_LEN)),\n",
    "    batch_size=batch_len,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model with regular Optimization\n",
    "\n",
    "In order to show, that memorization occurs, we will first train a simple 2-layer LSTM model and plot the estimated z-score. We will deactivate differentially private optimization for now, but we will define the network to allow differentially private optimization to make it more obvious that we are just changing the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpsgd = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a training hook to print epsilon values of the differentially private Adam after each epoch. This will not be important for now but used later when we train with differentially private Adam.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a training hook in order to be able to periodically print the epsilon values\n",
    "class EpsilonPrintingTrainingHook(tf.estimator.SessionRunHook):\n",
    "    \"\"\"Training hook to print current value of epsilon after an epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, ledger):\n",
    "        \"\"\"Initalizes the EpsilonPrintingTrainingHook.\n",
    "\n",
    "        Args:\n",
    "          ledger: The privacy ledger.\n",
    "        \"\"\"\n",
    "        self._samples, self._queries = ledger.get_unformatted_ledger()\n",
    "\n",
    "    def end(self, session):\n",
    "        orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "        samples = session.run(self._samples)\n",
    "        queries = session.run(self._queries)\n",
    "        formatted_ledger = privacy_ledger.format_ledger(samples, queries)\n",
    "        rdp = compute_rdp_from_ledger(formatted_ledger, orders)\n",
    "        eps = get_privacy_spent(orders, rdp, target_delta=1e-7)[0]\n",
    "        EPSILON_LIST.append(eps)\n",
    "        print('For delta=1e-7, the current epsilon is: %.2f' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using Tensorflow estimators\n",
    "def rnn_model_fn(features, labels, mode):  # pylint: disable=unused-argument\n",
    "    \"\"\"Model function for a RNN.\"\"\"\n",
    "\n",
    "    # Define RNN architecture using tf.keras.layers.\n",
    "    x = features['x']\n",
    "    input_layer = x[:]\n",
    "    input_one_hot = tf.one_hot(input_layer, 200)\n",
    "    lstm = tf.keras.layers.LSTM(200, return_sequences=True).apply(input_one_hot)\n",
    "    lstm = tf.keras.layers.LSTM(200, return_sequences=True).apply(lstm)\n",
    "    logits = tf.keras.layers.Dense(50).apply(lstm)\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
    "        vector_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.cast(tf.one_hot(labels, 50), dtype=tf.float32),\n",
    "            logits=logits)\n",
    "        # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
    "        scalar_loss = tf.reduce_mean(vector_loss)\n",
    "\n",
    "    # Configure the training op (for TRAIN mode).\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        if dpsgd:\n",
    "            ledger = privacy_ledger.PrivacyLedger(\n",
    "                population_size=NB_TRAIN,\n",
    "                selection_probability=(batch_size*SEQ_LEN / NB_TRAIN),\n",
    "                max_samples=1e6,\n",
    "                max_queries=1e6)\n",
    "            optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
    "                l2_norm_clip=l2_norm_clip,\n",
    "                noise_multiplier=noise_multiplier,\n",
    "                num_microbatches=microbatches,\n",
    "                learning_rate=learning_rate,\n",
    "                unroll_microbatches=True,\n",
    "                ledger=ledger)\n",
    "            training_hooks = [\n",
    "                EpsilonPrintingTrainingHook(ledger)\n",
    "            ]\n",
    "            opt_loss = vector_loss\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate)\n",
    "            training_hooks = []\n",
    "            opt_loss = scalar_loss\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=scalar_loss,\n",
    "                                          train_op=train_op,\n",
    "                                          training_hooks=training_hooks)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode).\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy':\n",
    "                tf.metrics.accuracy(\n",
    "                    labels=tf.cast(labels, dtype=tf.int32),\n",
    "                    predictions=tf.argmax(input=logits, axis=2))\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=scalar_loss,\n",
    "                                          eval_metric_ops=eval_metric_ops)\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=tf.nn.softmax(logits=logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_from = {'warm_start_from': model_dir_regular} if load_model else {}\n",
    "\n",
    "# Instantiate the tf.Estimator.\n",
    "conf = tf.estimator.RunConfig(save_summary_steps=1000)\n",
    "lm_classifier = tf.estimator.Estimator(model_fn=rnn_model_fn,\n",
    "                                        model_dir=model_dir_regular,\n",
    "                                       config=conf,\n",
    "                                       **warm_start_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define methods to estimate the memorization effect. For this we use the log-perplexity as a basis.\n",
    "\n",
    "Note: For simplicity, we ignored the influence of the first element on the log-perplexity. For our purpose this is irrelevant since, in our example, the first element of the sequence is fixed and therefore constitutes a constant offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the log-perplexity of a secret (at least approximately).\n",
    "def log_perplexity(estimator, sequence):\n",
    "    assert 0 < len(sequence.shape) <= 2, \"Length of the shape of the sequence has to be 1 or 2, currently it is {}\".\\\n",
    "        format(len(sequence.shape))\n",
    "    if len(sequence.shape) == 1:\n",
    "        formatted_sequence = sequence.reshape((1, -1))\n",
    "    else:\n",
    "        formatted_sequence = sequence\n",
    "    sequence_input = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'x': formatted_sequence},\n",
    "        batch_size=20,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    sequence_length = formatted_sequence.shape[1]\n",
    "    prediction_generator = estimator.predict(sequence_input)\n",
    "    log_perplexity_list = []\n",
    "    for i, prediction in enumerate(prediction_generator):\n",
    "        sequence_probabilities = prediction[(range(sequence_length-1), formatted_sequence[i, 1:])]\n",
    "        negative_log_probability = np.sum(-np.log(sequence_probabilities))\n",
    "        log_perplexity_list.append(negative_log_probability)\n",
    "    return log_perplexity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can estimate the z-score. In order to do this, we randomly sample 1,000 potential secrets and calculate their log-perplexities. These are approximately normal distributed. So we transform these and the log-perplexity of the actual secret to a standard normal distribution. Because of this, a low z-score of the secret corresponds to the secret being very probable under the model, indicating it was contained in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for estimating the z-score\n",
    "def estimate_z_score(estimator, secret, secret_format, dictionary, seed=42, sample_size=1000):\n",
    "    secret_log_perplexity = log_perplexity(estimator=estimator, sequence=secret)\n",
    "    np.random.seed(seed=seed)\n",
    "    samples_of_random_space = np.random.randint(0, 10, (sample_size, 9))\n",
    "    list_of_samples = []\n",
    "    for i in range(sample_size):\n",
    "        sample = secret_format.format(*samples_of_random_space[i]).split()\n",
    "        int_representation = [dictionary[character] for character in sample]\n",
    "        list_of_samples.append(int_representation)\n",
    "    sample_log_perplexity_list = log_perplexity(estimator, np.array(list_of_samples))\n",
    "    mean = np.mean(sample_log_perplexity_list)\n",
    "    std = np.std(sample_log_perplexity_list)\n",
    "    z_score = (secret_log_perplexity - mean)/std\n",
    "    return z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can finally train our model on the modified data set. We also print the z-scores after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "steps_per_epoch = NB_TRAIN // batch_len\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('epoch', epoch)\n",
    "    # Train the model for one epoch.\n",
    "    lm_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
    "\n",
    "    if epoch % 5 == 1:\n",
    "        name_input_fn = [('Train', train_input_fn), ('Eval', eval_input_fn)]\n",
    "        for name, input_fn in name_input_fn:\n",
    "            # Evaluate the model and print results\n",
    "            eval_results = lm_classifier.evaluate(input_fn=input_fn)\n",
    "            result_tuple = (epoch, eval_results['accuracy'], eval_results['loss'])\n",
    "            print(name, 'accuracy after %d epochs is: %.3f (%.4f)' % result_tuple)\n",
    "\n",
    "    z_score = estimate_z_score(estimator=lm_classifier,\n",
    "                                secret=secret_sequence,\n",
    "                                secret_format=dataset['secret_format'],\n",
    "                                dictionary=dataset['dictionary'],\n",
    "                                seed=seed + 1,\n",
    "                                sample_size=1000)\n",
    "    Z_SCORE_LIST.append(z_score)\n",
    "    print(\"z-score: {}\".format(z_score))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly save the z-scores for later use\n",
    "np.save('regular_z_scores.npy', np.array(Z_SCORE_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the memorization effect, we plot the z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting z-scores\n",
    "x = range(1, epoch + 1)\n",
    "plt.plot(x, Z_SCORE_LIST, label='z-score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('z-score')\n",
    "plt.legend()\n",
    "plt.title('Secret: {}'.format(dataset['secret_plain'].replace(' ', '').replace('_', ' ')))\n",
    "plt.savefig(\"z_score_{}_regular.png\".format(dataset['secret_format']).replace(' ', ''))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(Z_SCORE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the log-perplexity of potential secrets is approximiately a normal distribution.\n",
    "For visualization we plot a normal distribution and show where the current secret is placed, given the models log-perplexity for the secret. Here, the further to the left of the plot, the more probable the sequence is under the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1), label='Normal distribution')\n",
    "plt.scatter(Z_SCORE_LIST, stats.norm.pdf(Z_SCORE_LIST), marker='x', color='red', label='Secret Probabilities')\n",
    "plt.xlabel('Standard deviations from mean')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.title('Secret: {}'.format(dataset['secret_plain'].replace(' ', '').replace('_', ' ')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last plots we have seen that the secret is probable under the model and an attacker can assume that the training data contains the secret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model with Differentially Private Optimization\n",
    "\n",
    "For comparison, we will also train a model with differentially private optimization. This is noticeably slower and might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpsgd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_from = {'warm_start_from': model_dir_dp} if load_model else {}\n",
    "\n",
    "# Instantiate the tf.Estimator.\n",
    "conf = tf.estimator.RunConfig(save_summary_steps=1000)\n",
    "lm_classifier = tf.estimator.Estimator(model_fn=rnn_model_fn,\n",
    "                                        model_dir=model_dir_dp,\n",
    "                                       config=conf,\n",
    "                                       **warm_start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "steps_per_epoch = NB_TRAIN // batch_len\n",
    "Z_SCORE_LIST = []\n",
    "EPSILON_LIST = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('epoch', epoch)\n",
    "    # Train the model for one epoch.\n",
    "    lm_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
    "\n",
    "    if epoch % 5 == 1:\n",
    "        name_input_fn = [('Train', train_input_fn), ('Eval', eval_input_fn)]\n",
    "        for name, input_fn in name_input_fn:\n",
    "            # Evaluate the model and print results\n",
    "            eval_results = lm_classifier.evaluate(input_fn=input_fn)\n",
    "            result_tuple = (epoch, eval_results['accuracy'], eval_results['loss'])\n",
    "            print(name, 'accuracy after %d epochs is: %.3f (%.4f)' % result_tuple)\n",
    "\n",
    "    z_score = estimate_z_score(estimator=lm_classifier,\n",
    "                                secret=secret_sequence,\n",
    "                                secret_format=dataset['secret_format'],\n",
    "                                dictionary=dataset['dictionary'],\n",
    "                                seed=seed + 1,\n",
    "                                sample_size=1000)\n",
    "    Z_SCORE_LIST.append(z_score)\n",
    "    print(\"z-score: {}\".format(z_score))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly save the z-scores and epsilon values for later use\n",
    "np.save('dp_z_scores.npy', np.array(Z_SCORE_LIST))\n",
    "np.save('epsilon.npy', np.array(EPSILON_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we want to visualize the results again. We use the z-score again and a plot how probable the secret is under the model in comparison to other potential secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting z-scores\n",
    "x = range(1, epoch + 1)\n",
    "plt.plot(x, Z_SCORE_LIST, label='z-score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('z-score')\n",
    "plt.legend()\n",
    "plt.title('Secret: {}'.format(dataset['secret_plain'].replace(' ', '').replace('_', ' ')))\n",
    "plt.savefig(\"z_score_{}_dp.png\".format(dataset['secret_format']).replace(' ', ''))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(Z_SCORE_LIST)\n",
    "\n",
    "# If we are using DP Optimization, we want to plot the epsilons, too\n",
    "if dpsgd:\n",
    "    plt.plot(x, EPSILON_LIST, label='epsilon')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('epsilon')\n",
    "    plt.legend()\n",
    "    plt.title('Secret: {}'.format(dataset['secret_plain'].replace(' ', '').replace('_', ' ')))\n",
    "    plt.savefig(\"epsilon_{}_dp.png\".format(dataset['secret_format']).replace(' ', ''))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(EPSILON_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1), label='Normal distribution')\n",
    "plt.scatter(Z_SCORE_LIST, stats.norm.pdf(Z_SCORE_LIST), marker='x', color='red', label='Secret Probabilities')\n",
    "plt.xlabel('Standard deviations from mean')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.title('Secret: {}'.format(dataset['secret_plain'].replace(' ', '').replace('_', ' ')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will show that the z-score is closer to the mean of the potential secrets and stays within the standard deviation. Note, that the log-perplexity of the secret is neither consistently low nor high, but rather more or less randomly distributed. This causes an attacker to be unable to reliably infer if the secret has been contained in the training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
